# 完整的数据流场景实景说明

## 一、系统架构概览

### 1.1 缓存层级架构

```
┌─────────────────────────────────────────────────┐
│  Layer 1: Smart Cache + Common Cache (Redis)    │
│  存储：完整的API响应数据                         │
│  键例：receiver:get-stock-quote:700.HK:longport │
└─────────────────────────────────────────────────┘
                       ↓ 未命中
┌─────────────────────────────────────────────────┐
│  Layer 2: Symbol Mapper Cache (Memory LRU)      │
│  存储：符号映射关系（L3→L2→L1三层）              │
│  键例：symbol:longport:700.HK:to_standard       │
└─────────────────────────────────────────────────┘
                       ↓ 映射完成
┌─────────────────────────────────────────────────┐
│  Layer 3: Business Logic Flow                   │
│  Data Fetcher → Transformer → Storage           │
└─────────────────────────────────────────────────┘
```

### 1.2 关键特性

- **双层缓存体系**：Smart Cache（业务数据） + Symbol Mapper Cache（映射关系）
- **独立运行**：两个缓存系统完全独立，互不干扰
- **动态TTL**：由策略与数据类型动态计算；在 MARKET_AWARE 策略下按市场状态调整
- **Smart Cache 写入**：在请求内完成（后台更新为独立机制）
- **并发优化**：相同请求只执行一次，其他等待结果

## 二、完整数据流场景

### 场景1：Smart Cache 命中（最优路径）

```
用户请求
  ↓
Receiver.handleRequest()
  ↓
Smart Cache 检查 [receiver:get-stock-quote:700.HK]
  ✅ 命中
  ↓
直接返回缓存数据 → 用户
```

**特点**：
- 响应时间：~10ms
- 资源消耗：最小（仅Redis查询）
- 适用场景：热门股票、高频查询
- 命中率：生产环境90%+

### 场景2：Smart Cache 未命中 + Symbol Cache 全部命中

```
用户请求
  ↓
Receiver.handleRequest()
  ↓
Smart Cache 检查
  ❌ 未命中
  ↓
executeOriginalDataFlow()
  ↓
Symbol Mapper
  ├→ L3批量缓存 ✅ 命中
  └→ 返回映射结果 {700.HK→00700}
  ↓
Data Fetcher (使用映射后符号)
  ↓
Transformer (数据标准化/转换)
  ↓
Storage (异步存储)
  ↓
Smart Cache 写入（请求内）
  ↓
返回数据 → 用户
```

**特点**：
- 响应时间：~100-200ms
- 资源消耗：中等（需调用Provider API）
- 适用场景：缓存过期但映射关系稳定
- 发生频率：约8-10%

### 场景3：Smart Cache 未命中 + Symbol Cache 部分命中

```
用户请求 [700.HK, AAPL, 00001.HK]
  ↓
Receiver.handleRequest()
  ↓
Smart Cache 检查
  ❌ 未命中
  ↓
executeOriginalDataFlow()
  ↓
Symbol Mapper
  ├→ L3批量缓存 ❌ 未命中
  ├→ L2单符号缓存
  │   ├→ 700.HK ✅ 命中 → 00700
  │   ├→ AAPL ✅ 命中 → AAPL  
  │   └→ 00001.HK ❌ 未命中
  ├→ L1规则缓存（处理未命中的00001.HK）
  │   └→ 查询MongoDB规则 → 应用转换
  └→ 合并结果 + 回填L2缓存
  ↓
Data Fetcher (批量获取)
  ↓
Transformer (数据标准化/转换)
  ↓
Storage
  ↓
Smart Cache 写入（请求内）
  ↓
返回数据 → 用户
```

**特点**：
- 响应时间：~200-300ms
- 资源消耗：较高（MongoDB查询 + Provider API）
- 适用场景：混合查询（热门+冷门股票）
- 优化策略：自动回填缓存，下次查询更快

### 场景4：全部缓存未命中（冷启动）

```
用户请求
  ↓
Receiver.handleRequest()
  ↓
Smart Cache 检查
  ❌ 未命中
  ↓
executeOriginalDataFlow()
  ↓
Symbol Mapper
  ├→ L3批量缓存 ❌
  ├→ L2单符号缓存 ❌
  ├→ L1规则缓存 ❌
  ├→ MongoDB查询映射规则
  ├→ 应用规则转换所有符号
  ├→ 回填L1规则缓存
  ├→ 回填L2符号缓存
  └→ 存储L3批量结果
  ↓
Data Fetcher (调用Provider API)
  ↓
Transformer (数据标准化/转换)
  ↓
Storage (Redis + MongoDB)
  ↓
Smart Cache 写入（请求内）
  ↓
返回数据 → 用户
```

**特点**：
- 响应时间：~500-1000ms
- 资源消耗：最高（全链路执行）
- 适用场景：系统启动、新股票、缓存清空
- 发生频率：<1%（仅冷启动或新数据）

## 三、缓存TTL动态决策机制

### 3.1 Smart Cache TTL策略

说明：以下为示例范围，实际 TTL 由 CommonCacheService.calculateOptimalTTL 动态计算，并受缓存策略影响（如 STRONG_TIMELINESS、MARKET_AWARE）；Receiver 默认使用 STRONG_TIMELINESS 策略。

```typescript
Smart Cache TTL (动态计算)
  ├→ 基于市场状态
  │   ├→ 开市时间：1-5秒
  │   ├→ 收市时间：30-300秒
  │   └→ 周末/假期：3600秒
  ├→ 基于数据类型
  │   ├→ 实时行情：1-5秒
  │   ├→ 基本信息：300-3600秒
  │   └→ 历史数据：3600-86400秒
  └→ 基于缓存策略
      ├→ STRONG_TIMELINESS：1-5秒
      ├→ MARKET_AWARE：动态调整
      └→ LONG_TERM：3600+秒
```

### 3.2 Symbol Mapper Cache TTL配置

```typescript
Symbol Mapper Cache TTL (配置驱动)
  ├→ L1规则：featureFlags.ruleCacheTtl（默认300秒）
  ├→ L2符号：featureFlags.symbolCacheTtl（默认600秒）
  └→ L3批量：featureFlags.batchResultCacheTtl（默认300秒）
```

### 3.3 TTL计算示例代码

```typescript
// CommonCacheService中的动态TTL计算
const ttlResult = CommonCacheService.calculateOptimalTTL({
  symbol: '700.HK',
  dataType: 'quote',
  marketStatus: {
    isOpen: true,  // 港股开市中
    timezone: 'Asia/Hong_Kong'
  },
  freshnessRequirement: 'high' // 基于策略映射
});
// 开市期间返回：5秒
// 收市期间返回：300秒
// 周末返回：3600秒
```

## 四、缓存失效与更新机制

### 4.1 主动失效链

```
映射规则更新（管理员操作）
  ↓
Symbol Mapper Cache失效
  ├→ L1规则缓存清除（provider维度）
  ├→ L2相关符号缓存清除（精准失效）
  └→ L3包含这些符号的批量缓存清除
  ↓
Smart Cache相关键失效（可选，通过事件触发）
```

### 4.2 自动过期机制

- **TTL过期**：缓存到达生命周期自动删除
- **LRU淘汰**：内存缓存满时淘汰最少使用项
- **版本控制**：通过版本号实现缓存更新

## 五、关键组件职责矩阵

| 组件 | 职责 | 缓存内容 | 存储介质 | TTL策略 |
|-----|------|---------|---------|---------|
| **Smart Cache** | 请求级缓存 | 完整API响应 | Redis | 动态计算 |
| **Common Cache** | 通用缓存服务 | Smart Cache的存储层 | Redis | 动态计算 |
| **Symbol Mapper L3** | 批量映射缓存 | 批量符号映射结果 | Memory | 配置驱动 |
| **Symbol Mapper L2** | 单符号缓存 | 单个符号映射 | Memory | 配置驱动 |
| **Symbol Mapper L1** | 规则缓存 | Provider映射规则 | Memory | 配置驱动 |
| **Data Fetcher** | 数据获取 | 无缓存 | - | - |
| **Transformer** | 数据标准化/转换 | 无缓存 | - | - |
| **Storage** | 持久化存储 | 原始+转换后数据 | Redis+MongoDB | 配置驱动 |

## 六、性能指标与优化

### 6.1 关键性能指标

| 场景 | 响应时间 | 缓存命中率 | CPU消耗 | 网络IO |
|-----|---------|-----------|---------|---------|
| Smart Cache命中 | ~10ms | 90%+ | 极低 | 仅Redis |
| Symbol Cache命中 | ~100-200ms | 70% | 低 | Provider API |
| 部分命中 | ~200-300ms | 30% | 中 | MongoDB+API |
| 冷启动 | ~500-1000ms | 0% | 高 | 全链路 |

### 6.2 优化策略

1. **Smart Cache优先**
   - 90%+请求直接命中
   - 响应时间10ms级别
   - 减少下游压力

2. **Symbol Cache分层**
   - L3批量缓存减少重复计算
   - L2单符号缓存提高命中率
   - L1规则缓存减少MongoDB查询

3. **并发控制**
   - pendingQueries机制防止重复查询
   - 相同请求只执行一次
   - 其他请求等待结果

4. **异步写入**
   - 缓存写入不阻塞主流程
   - 提高响应速度
   - 降低延迟

5. **智能TTL**
   - 根据市场状态动态调整
   - 开市短缓存保证实时性
   - 收市长缓存减少API调用

## 七、监控与告警

### 7.1 关键监控指标

- **缓存命中率**：Smart Cache > 90%, Symbol Cache > 70%
- **响应时间**：P95 < 200ms, P99 < 500ms
- **错误率**：< 0.1%
- **API调用次数**：监控Provider API调用频率

### 7.2 告警阈值

- 缓存命中率低于80%
- 响应时间P95超过500ms
- 错误率超过1%
- MongoDB查询超时

## 八、Query组件数据流分析

### 8.1 Query组件架构特点

Query组件是系统的**长时效查询接口**，与Receiver组件形成互补：

| 特性 | Query组件 | Receiver组件 |
|-----|----------|-------------|
| **定位** | 长时效查询接口 | 强时效实时接口 |
| **缓存策略** | WEAK_TIMELINESS（弱时效） | STRONG_TIMELINESS（强时效） |
| **缓存TTL** | 长TTL（策略驱动，默认约300秒） | 5秒 |
| **使用场景** | 批量查询，容忍延迟 | 实时行情，要求最新 |
| **缓存键前缀** | 当前实现沿用 `receiver:`（将来可独立 `query:`） | `receiver:` |
| **批处理** | 支持大批量并行 | 单个或小批量 |

### 8.2 Query组件数据流

#### 场景1：Query Smart Cache命中（最优）

```
用户批量查询请求
  ↓
Query.executeQuery()
  ↓
Query Smart Cache检查（弱时效，长TTL，策略驱动）
  ✅ 全部命中
  ↓
直接返回缓存数据 → 用户
```

**特点**：
- 响应时间：~10ms
- 缓存策略：WEAK_TIMELINESS
- 适用场景：报表查询、历史数据分析

#### 场景2：Query缓存未命中，Receiver缓存命中

```
用户批量查询请求
  ↓
Query.executeQuery()
  ↓
Query Smart Cache检查（弱时效，长TTL，策略驱动）
  ❌ 未命中
  ↓
调用 receiverService.handleRequest()
  ↓
Receiver Smart Cache检查 [receiver层5秒缓存]
  ✅ 命中
  ↓
返回数据
  ↓
写入Query Smart Cache（长TTL，策略驱动）
  ↓
返回数据 → 用户
```

**特点**：
- 响应时间：~50-100ms
- 双层缓存协同
- 下次查询将命中Query层缓存

#### 场景3：双层缓存都未命中

```
用户批量查询请求
  ↓
Query.executeQuery()
  ↓
Query Smart Cache检查（弱时效，长TTL，策略驱动）
  ❌ 未命中
  ↓
调用 receiverService.handleRequest()
  ↓
Receiver Smart Cache检查（5秒）
  ❌ 未命中
  ↓
执行完整数据流
  ├→ Symbol Mapper（三层缓存）
  ├→ Data Fetcher（Provider API）
  ├→ Transformer（数据标准化）
  └→ Storage（持久化）
  ↓
写入Receiver Smart Cache（5秒）
  ↓
写入Query Smart Cache（长TTL，策略驱动）
  ↓
返回数据 → 用户
```

**特点**：
- 响应时间：~200-500ms
- 完整数据流执行
- 双层缓存都会更新

### 8.3 Query批量处理优化

Query组件专门优化了批量查询场景：

```typescript
// 批量处理管道
executeBatchedPipeline()
  ├→ 按市场分组（HK、US、SH、SZ）
  ├→ 市场级并行处理
  │   └→ 每个市场再分片（MAX_MARKET_BATCH_SIZE）
  │       └→ 分片级并行处理
  │           └→ Receiver批量调用（MAX_BATCH_SIZE）
  └→ 结果合并与返回
```

**并行处理特点**：
- 三级并行：市场级 → 分片级 → Receiver批量级
- 超时控制：每级都有独立超时机制
- 错误隔离：单个失败不影响其他批次

### 8.4 缓存协同工作机制

```
┌─────────────────────────────────────────────────┐
│         Query Smart Cache (300秒)               │
│  场景：批量查询、报表、非实时数据                  │
└─────────────────────────────────────────────────┘
                       ↓ 未命中时调用
┌─────────────────────────────────────────────────┐
│        Receiver Smart Cache (5秒)               │
│  场景：实时行情、交易数据、最新价格               │
└─────────────────────────────────────────────────┘
                       ↓ 未命中时执行
┌─────────────────────────────────────────────────┐
│  Symbol Mapper → Data Fetcher → Transformer     │
└─────────────────────────────────────────────────┘
```

**关键设计理念**：
1. **分层缓存**：不同时效要求使用不同缓存层
2. **代码复用**：Query直接调用Receiver服务，避免重复实现
3. **独立控制**：每层可独立调整缓存策略
4. **智能降级**：上层缓存未命中自动降级到下层

### 8.5 性能指标对比

| 场景 | Query组件响应时间 | Receiver组件响应时间 |
|-----|------------------|-------------------|
| L1缓存命中 | ~10ms | ~10ms |
| L2缓存命中 | ~50-100ms | ~100-200ms |
| 缓存全未命中 | ~200-500ms | ~500-1000ms |
| 缓存命中率 | 95%+（5分钟TTL） | 70%+（5秒TTL） |

## 九、总结

本系统通过**三层架构设计**实现了高性能的股票数据服务：

1. **应用层分离**：
   - Query组件：长时效查询接口（300秒缓存）
   - Receiver组件：强时效实时接口（5秒缓存）

2. **缓存层独立**：
   - Smart Cache：缓存完整业务数据（Redis）
   - Symbol Mapper Cache：缓存符号映射（内存LRU）

3. **数据层统一**：
   - 共享底层数据获取流程
   - Data Fetcher → Transformer → Storage

通过这种分层设计，系统在不同场景下都能提供最优性能：
- **实时交易**：Receiver组件5秒缓存，保证数据新鲜度
- **批量查询**：Query组件5分钟缓存，减少API调用
- **符号映射**：独立三层缓存，最大化复用
- **并发优化**：多级并行处理，提高吞吐量

整个架构充分考虑了金融数据的不同时效要求，通过智能缓存策略和分层架构，在保证数据准确性的同时，最大化地提升了系统性能。